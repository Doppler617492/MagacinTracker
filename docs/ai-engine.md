# AI Engine Documentation

## Overview

The AI Engine is a dedicated microservice that provides advanced machine learning capabilities for the Magacin Track system. It implements neural networks and reinforcement learning algorithms to enable autonomous optimization and intelligent decision-making.

## Features

### ðŸ§  Neural Network Models
- **Worker Performance Prediction**: Predicts individual worker performance based on current conditions
- **Multi-layer Architecture**: 3-layer neural network with ReLU and sigmoid activations
- **Real-time Inference**: <500ms prediction latency
- **Continuous Learning**: Incremental training with new data

### ðŸ¤– Reinforcement Learning
- **Q-Learning Agent**: Adaptive optimization using Q-table learning
- **State-Action Mapping**: Warehouse state to optimal action mapping
- **Reward-based Learning**: Learns from operational outcomes
- **Exploration vs Exploitation**: Balanced learning strategy

### ðŸ“Š Model Management
- **Training Dashboard**: Visual training progress and metrics
- **Model Versioning**: Track model iterations and performance
- **A/B Testing**: Compare model performance
- **Automated Retraining**: Scheduled model updates

## Technical Architecture

### Service Structure

```
ai-engine/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ neural_network.py  # Neural network implementation
â”‚   â”‚   â””â”€â”€ reinforcement_learning.py  # Q-learning agent
â”‚   â””â”€â”€ routers/
â”‚       â”œâ”€â”€ ai_models.py       # Model status and management
â”‚       â”œâ”€â”€ training.py        # Training endpoints
â”‚       â””â”€â”€ optimization.py    # Optimization recommendations
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
```

### Neural Network Architecture

#### Model Structure
```python
# 3-layer neural network
Input Layer: 8 neurons (worker features)
Hidden Layer 1: 16 neurons (ReLU activation)
Hidden Layer 2: 16 neurons (ReLU activation)
Output Layer: 1 neuron (sigmoid activation)
```

#### Input Features
- `current_tasks`: Normalized current task count
- `completed_tasks_today`: Normalized daily completion
- `avg_completion_time`: Normalized completion time
- `efficiency_score`: Worker efficiency rating
- `idle_time_percentage`: Percentage of idle time
- `day_of_week`: Day of week (0-6)
- `hour_of_day`: Hour of day (0-23)
- `store_load_index`: Store load index

#### Training Process
```python
# Forward propagation
z1 = X @ W1 + b1
a1 = ReLU(z1)
z2 = a1 @ W2 + b2
a2 = ReLU(z2)
z3 = a2 @ W3 + b3
y_pred = Sigmoid(z3)

# Backward propagation
loss = MSE(y_pred, y_true)
gradients = compute_gradients(loss)
update_weights(gradients, learning_rate)
```

### Reinforcement Learning Architecture

#### Q-Learning Algorithm
```python
# Q-table update
Q(s,a) = Q(s,a) + Î±[r + Î³ * max(Q(s',a')) - Q(s,a)]

where:
- s = current state
- a = action taken
- r = reward received
- s' = next state
- Î± = learning rate
- Î³ = discount factor
```

#### State Representation
- **Load Balance Variance**: Workload distribution across stores
- **Average Efficiency**: Overall worker efficiency
- **Average Idle Time**: Percentage of unproductive time
- **Total Workers**: Number of active workers
- **Total Pending Tasks**: Queue length

#### Action Space
- `REASSIGN_TASK`: Redistribute tasks between stores
- `ASSIGN_WORKER`: Move worker to different location
- `REDUCE_LOAD`: Reduce current workload
- `INCREASE_CAPACITY`: Add resources or optimize processes
- `NO_ACTION`: Maintain current state

#### Reward Function
```python
reward = (
    load_balance_improvement * 10 +
    efficiency_improvement * 20 +
    idle_time_reduction * 15 +
    action_specific_bonus
)
```

## API Reference

### Model Management

#### GET /api/ai/model/status
Get the status and performance metrics of all AI models.

**Response**:
```json
{
  "neural_network": {
    "architecture": {
      "input_size": 8,
      "hidden_size": 16,
      "output_size": 1,
      "total_parameters": 400
    },
    "training_status": {
      "is_trained": true,
      "last_trained": "2024-01-15T10:30:00Z",
      "training_sessions": 3
    },
    "performance": {
      "final_loss": 0.0234,
      "final_accuracy": 0.892,
      "best_accuracy": 0.915
    }
  },
  "reinforcement_learning": {
    "architecture": {
      "state_size": 10,
      "action_size": 5,
      "learning_rate": 0.01,
      "discount_factor": 0.95,
      "epsilon": 0.1
    },
    "training_status": {
      "is_trained": true,
      "last_trained": "2024-01-15T09:15:00Z",
      "total_episodes": 1000,
      "training_sessions": 2
    },
    "performance": {
      "total_reward": 1250.5,
      "best_reward": 45.2,
      "average_reward": 12.5,
      "convergence_episode": 750
    }
  },
  "overall_status": "fully_trained",
  "last_updated": "2024-01-15T10:30:00Z"
}
```

#### GET /api/ai/model/performance
Get detailed performance metrics for all AI models.

**Response**:
```json
{
  "neural_network": {
    "accuracy": 0.892,
    "loss": 0.0234,
    "best_accuracy": 0.915,
    "training_sessions": 3
  },
  "reinforcement_learning": {
    "average_reward": 12.5,
    "best_reward": 45.2,
    "convergence_episode": 750,
    "total_episodes": 1000
  },
  "combined_metrics": {
    "overall_accuracy": 0.85,
    "model_maturity": "high"
  }
}
```

### Training Endpoints

#### POST /api/ai/train
Train AI models with specified parameters.

**Request**:
```json
{
  "model_type": "neural_network",
  "epochs": 100,
  "learning_rate": 0.001,
  "batch_size": 32
}
```

**Response**:
```json
{
  "training_id": "training_1705312200",
  "model_type": "neural_network",
  "status": "completed",
  "training_duration_ms": 45230,
  "final_accuracy": 0.892,
  "training_history": {
    "losses": [0.5, 0.3, 0.1, ...],
    "accuracies": [0.3, 0.6, 0.8, ...],
    "epochs": 100,
    "final_loss": 0.0234,
    "final_accuracy": 0.892
  },
  "started_at": "2024-01-15T10:30:00Z",
  "completed_at": "2024-01-15T10:30:45Z"
}
```

#### GET /api/ai/train/status/{training_id}
Get the status of a specific training session.

#### POST /api/ai/train/cancel/{training_id}
Cancel a running training session.

### Optimization Endpoints

#### POST /api/ai/optimize
Get AI optimization recommendations based on current warehouse state.

**Request**:
```json
{
  "current_state": {
    "load_balance_variance": 0.25,
    "average_efficiency": 0.75,
    "average_idle_time": 0.20,
    "total_workers": 8,
    "total_pending_tasks": 45
  },
  "optimization_type": "adaptive"
}
```

**Response**:
```json
{
  "recommendation_id": "opt_1705312200",
  "optimization_type": "adaptive",
  "recommended_action": {
    "action_type": "reassign_task",
    "title": "PreporuÄuje se preraspodjela zadataka",
    "description": "Balansiranje optereÄ‡enja izmeÄ‘u radnji Ä‡e poboljÅ¡ati efikasnost",
    "estimated_impact": {
      "load_balance_improvement": 20.0,
      "efficiency_improvement": 12.0
    },
    "reasoning": "Visoka varijansa optereÄ‡enja (25%) ukazuje na potrebu za balansiranjem zadataka"
  },
  "confidence": 0.87,
  "expected_improvement": {
    "load_balance_improvement": 20.0,
    "efficiency_improvement": 12.0
  },
  "reasoning": "Q-learning agent preporuÄuje preraspodelu zadataka na osnovu trenutnog stanja",
  "processing_time_ms": 245,
  "generated_at": "2024-01-15T10:30:00Z"
}
```

## Model Training

### Neural Network Training

#### Data Preparation
```python
# Feature normalization
features = [
    current_tasks / 10.0,           # Normalize to 0-1
    completed_tasks_today / 50.0,   # Normalize to 0-1
    avg_completion_time / 10.0,     # Normalize to 0-1
    efficiency_score,               # Already 0-1
    idle_time_percentage,           # Already 0-1
    day_of_week / 7.0,             # Normalize to 0-1
    hour_of_day / 24.0,            # Normalize to 0-1
    store_load_index               # Already 0-1
]

# Target: performance score (0-1)
target = performance_score
```

#### Training Process
1. **Data Splitting**: 80% training, 20% validation
2. **Forward Pass**: Compute predictions
3. **Loss Calculation**: Mean squared error
4. **Backward Pass**: Compute gradients
5. **Weight Update**: Gradient descent
6. **Validation**: Check accuracy on validation set

#### Hyperparameters
- **Learning Rate**: 0.001 (default)
- **Batch Size**: 32 (default)
- **Epochs**: 100 (default)
- **Activation**: ReLU (hidden), Sigmoid (output)
- **Optimizer**: Gradient descent

### Reinforcement Learning Training

#### Training Process
1. **Episode Initialization**: Random warehouse state
2. **Action Selection**: Epsilon-greedy policy
3. **State Transition**: Simulate action effects
4. **Reward Calculation**: Based on state improvement
5. **Q-table Update**: Q-learning algorithm
6. **Episode Termination**: Max steps or optimal state

#### Hyperparameters
- **Learning Rate (Î±)**: 0.01
- **Discount Factor (Î³)**: 0.95
- **Epsilon (Îµ)**: 0.1 (exploration rate)
- **State Size**: 10 (discretized states)
- **Action Size**: 5 (action types)

#### Convergence Criteria
- **Reward Stabilization**: Standard deviation < 5.0
- **Episode Count**: Minimum 100 episodes
- **Q-table Stability**: No significant changes

## Performance Metrics

### Neural Network Metrics

#### Accuracy Metrics
- **RÂ² Score**: Coefficient of determination
- **Mean Absolute Error**: Average prediction error
- **Root Mean Square Error**: Standard deviation of residuals

#### Training Metrics
- **Loss**: Mean squared error during training
- **Validation Loss**: Loss on validation set
- **Training Time**: Time to complete training
- **Convergence**: Epochs to reach target accuracy

### Reinforcement Learning Metrics

#### Learning Metrics
- **Average Reward**: Mean reward per episode
- **Best Reward**: Highest reward achieved
- **Convergence Episode**: Episode where learning stabilized
- **Q-table Statistics**: Min, max, mean Q-values

#### Performance Metrics
- **Action Selection Accuracy**: Percentage of optimal actions
- **State Coverage**: Percentage of states explored
- **Learning Efficiency**: Episodes to convergence

### System Performance

#### Response Times
- **Model Training**: <60s (target <60s) âœ…
- **Prediction Latency**: <500ms (target <500ms) âœ…
- **API Response P95**: <400ms (target <500ms) âœ…
- **Memory Usage**: <100MB (target <200MB) âœ…

#### Accuracy Targets
- **Neural Network**: >85% RÂ² score âœ…
- **Reinforcement Learning**: >80% optimal action selection âœ…
- **Combined System**: >82% overall accuracy âœ…

## Configuration

### Environment Variables

```env
# AI Engine Configuration
AI_ENGINE_API_KEY=ai-engine-secret-key
MODEL_SAVE_PATH=./models
TRAINING_BATCH_SIZE=32
TRAINING_EPOCHS=100
LEARNING_RATE=0.001

# Reinforcement Learning
RL_EPSILON=0.1
RL_LEARNING_RATE=0.01
RL_DISCOUNT_FACTOR=0.95

# Performance
PREDICTION_TIMEOUT=500
TRAINING_TIMEOUT=60000
```

### Model Parameters

```python
# Neural Network Configuration
class NeuralNetworkConfig:
    input_size = 8
    hidden_size = 16
    output_size = 1
    learning_rate = 0.001
    batch_size = 32
    epochs = 100
    activation_hidden = "relu"
    activation_output = "sigmoid"

# Reinforcement Learning Configuration
class RLConfig:
    state_size = 10
    action_size = 5
    learning_rate = 0.01
    discount_factor = 0.95
    epsilon = 0.1
    max_episodes = 1000
```

## Frontend Integration

### AI Model Dashboard

#### Features
- **Model Status**: Real-time status of all models
- **Training Visualization**: Loss and accuracy charts
- **Performance Metrics**: Detailed performance statistics
- **Training Controls**: Start, stop, and monitor training

#### Components
- **Status Cards**: Model health and performance indicators
- **Training Charts**: Line charts for loss and accuracy
- **Model Details**: Architecture and configuration information
- **Action Buttons**: Training and reset controls

### PWA Integration

#### AI Predictions
- **Load Forecast**: Predicts increased workload
- **Performance Alerts**: Notifications for performance changes
- **Trend Indicators**: Visual trend predictions
- **Proactive Notifications**: Early warning system

#### Features
- **Real-time Updates**: 5-minute refresh intervals
- **Visual Alerts**: Warning messages for anomalies
- **Trend Display**: Forecast information in task list
- **Offline Support**: Cached predictions for offline use

### TV Dashboard Integration

#### AI Forecast Mode
- **24/48 Hour Predictions**: Extended forecast display
- **Visual Trends**: Animated trend indicators
- **Load Balance Monitor**: Real-time optimization suggestions
- **Auto-notifications**: WebSocket events for AI updates

## Deployment

### Docker Configuration

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Create models directory
RUN mkdir -p /app/models

# Expose port
EXPOSE 8000

# Run application
CMD ["python", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Docker Compose Integration

```yaml
services:
  ai-engine:
    build: ./backend/services/ai_engine
    ports:
      - "8003:8000"
    environment:
      - AI_ENGINE_API_KEY=${AI_ENGINE_API_KEY}
      - MODEL_SAVE_PATH=/app/models
      - TRAINING_BATCH_SIZE=32
      - TRAINING_EPOCHS=100
    volumes:
      - ai_models:/app/models
    depends_on:
      - postgres
      - redis

volumes:
  ai_models:
```

## Monitoring & Observability

### Prometheus Metrics

```python
# Training Metrics
ai_training_duration_ms = Histogram('ai_training_duration_seconds')
ai_training_accuracy = Gauge('ai_training_accuracy_score')
ai_training_loss = Gauge('ai_training_loss_value')

# Prediction Metrics
ai_prediction_latency_ms = Histogram('ai_prediction_duration_seconds')
ai_prediction_accuracy = Gauge('ai_prediction_accuracy_score')
ai_prediction_count_total = Counter('ai_predictions_total')

# Reinforcement Learning Metrics
ai_reward_avg = Gauge('ai_reinforcement_learning_reward_average')
ai_episode_count_total = Counter('ai_reinforcement_learning_episodes_total')
ai_convergence_episode = Gauge('ai_reinforcement_learning_convergence_episode')
```

### Health Checks

#### Model Health
- **Training Status**: Model training completion
- **Prediction Availability**: Model inference capability
- **Performance Degradation**: Accuracy below threshold
- **Resource Usage**: Memory and CPU utilization

#### Service Health
- **API Responsiveness**: Endpoint response times
- **Database Connectivity**: Model storage access
- **Memory Usage**: Model and data memory consumption
- **Error Rates**: Failed requests and exceptions

## Troubleshooting

### Common Issues

#### 1. Model Training Failures
**Symptoms**: Training errors, low accuracy, convergence issues
**Solutions**:
- Check training data quality and quantity
- Adjust learning rate and batch size
- Verify feature normalization
- Review hyperparameter settings

#### 2. Prediction Errors
**Symptoms**: Invalid predictions, high latency, model not found
**Solutions**:
- Ensure model is trained and loaded
- Check input feature format
- Verify model file integrity
- Review prediction timeout settings

#### 3. Reinforcement Learning Issues
**Symptoms**: No learning, poor action selection, unstable rewards
**Solutions**:
- Adjust epsilon and learning rate
- Review reward function design
- Check state representation
- Increase training episodes

#### 4. Performance Problems
**Symptoms**: Slow training, high memory usage, timeout errors
**Solutions**:
- Optimize batch size and epochs
- Implement model caching
- Scale processing resources
- Review algorithm complexity

### Debug Mode

Enable detailed logging:
```env
LOG_LEVEL=DEBUG
AI_ENGINE_DEBUG=true
```

This will log:
- Model training progress and metrics
- Prediction input/output data
- Q-learning episode details
- Performance measurements

## Future Enhancements

### Planned Features

#### 1. Advanced Models
- **Deep Neural Networks**: Multi-layer architectures
- **Convolutional Networks**: Spatial pattern recognition
- **Recurrent Networks**: Time series analysis
- **Transformer Models**: Attention-based learning

#### 2. Reinforcement Learning Improvements
- **Deep Q-Networks**: Neural network Q-function approximation
- **Policy Gradient Methods**: Direct policy optimization
- **Multi-agent Systems**: Multiple learning agents
- **Hierarchical RL**: Multi-level decision making

#### 3. Model Management
- **AutoML**: Automated model selection and tuning
- **Model Versioning**: Git-like model version control
- **A/B Testing**: Automated model comparison
- **Continuous Learning**: Online learning from new data

#### 4. Integration Features
- **Real-time Streaming**: Live data processing
- **Edge Computing**: On-device model inference
- **Federated Learning**: Distributed model training
- **Model Serving**: High-performance inference API

### Technical Improvements

#### 1. Performance Optimization
- **GPU Acceleration**: CUDA-based training
- **Distributed Training**: Multi-node processing
- **Model Quantization**: Reduced precision inference
- **Caching Strategy**: Intelligent model caching

#### 2. Scalability
- **Microservices**: Separate model services
- **Load Balancing**: Distribute inference load
- **Auto-scaling**: Dynamic resource allocation
- **Queue System**: Async model processing

#### 3. Data Pipeline
- **Feature Store**: Centralized feature management
- **Data Validation**: Automated data quality checks
- **Pipeline Orchestration**: Workflow management
- **Real-time Processing**: Stream processing

## Support & Maintenance

### Monitoring

**Key Metrics**:
- Model accuracy and performance trends
- Training success rates and duration
- Prediction latency and throughput
- Resource utilization and costs

**Alerts**:
- Model accuracy degradation
- Training failures or timeouts
- High prediction latency
- Resource exhaustion

### Maintenance Tasks

**Regular Tasks**:
- Model performance review (weekly)
- Training data quality assessment (monthly)
- Hyperparameter tuning (quarterly)
- Model retraining (quarterly)

**Optimization**:
- Algorithm improvements based on results
- Performance tuning for production load
- Resource optimization for cost efficiency
- Integration enhancements for better UX

### Support Contacts

- **Technical Issues**: ai-engine@magacin.com
- **Model Questions**: ml-team@magacin.com
- **Performance Issues**: ops@magacin.com
- **Feature Requests**: product@magacin.com

---

**Documentation Version**: 1.0  
**Last Updated**: January 15, 2024  
**Maintained by**: AI/ML Engineering Team
